{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dropout: float=0.1, \n",
    "        max_seq_len: int=5000, \n",
    "        d_model: int=512,\n",
    "        batch_first: bool=False\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.batch_first = batch_first\n",
    "        self.x_dim = 1 if batch_first else 0\n",
    "\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x + self.pe[:x.size(self.x_dim)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        input_size: int,\n",
    "        dec_seq_len: int,\n",
    "        batch_first: bool,\n",
    "        out_seq_len: int=58,\n",
    "        dim_val: int=512,  \n",
    "        n_encoder_layers: int=4,\n",
    "        n_decoder_layers: int=4,\n",
    "        n_heads: int=8,\n",
    "        dropout_encoder: float=0.2, \n",
    "        dropout_decoder: float=0.2,\n",
    "        dropout_pos_enc: float=0.1,\n",
    "        dim_feedforward_encoder: int=2048,\n",
    "        dim_feedforward_decoder: int=2048,\n",
    "        num_predicted_features: int=1,\n",
    "        max_seq_len: int=5000\n",
    "        ): \n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "        \n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features=input_size, \n",
    "            out_features=dim_val \n",
    "            )\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features=num_predicted_features,\n",
    "            out_features=dim_val\n",
    "            )  \n",
    "        \n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features=dim_val, \n",
    "            out_features=num_predicted_features\n",
    "            )\n",
    "\n",
    "        self.positional_encoding_layer = PositionalEncoder(\n",
    "            d_model=dim_val,\n",
    "            dropout=dropout_pos_enc\n",
    "            )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout_decoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=n_decoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, \n",
    "                tgt_mask: Tensor=None) -> Tensor:\n",
    "\n",
    "        src = self.encoder_input_layer(src) \n",
    "        src = self.positional_encoding_layer(src) \n",
    "        src = self.encoder( \n",
    "            src=src\n",
    "            )\n",
    "        decoder_output = self.decoder_input_layer(tgt) \n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "            )\n",
    "        decoder_output = self.linear_mapping(decoder_output) \n",
    "        return decoder_output\n",
    "    \n",
    "class TransformerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class used for transformer models.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        data: torch.tensor,\n",
    "        indices: list, \n",
    "        enc_seq_len: int, \n",
    "        dec_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            data: tensor, the entire train, validation or test data sequence \n",
    "                        before any slicing. If univariate, data.size() will be \n",
    "                        [number of samples, number of variables]\n",
    "                        where the number of variables will be equal to 1 + the number of\n",
    "                        exogenous variables. Number of exogenous variables would be 0\n",
    "                        if univariate.\n",
    "\n",
    "            indices: a list of tuples. Each tuple has two elements:\n",
    "                     1) the start index of a sub-sequence\n",
    "                     2) the end index of a sub-sequence. \n",
    "                     The sub-sequence is split into src, trg and trg_y later.  \n",
    "\n",
    "            enc_seq_len: int, the desired length of the input sequence given to the\n",
    "                     the first layer of the transformer model.\n",
    "\n",
    "            target_seq_len: int, the desired length of the target sequence (the output of the model)\n",
    "\n",
    "            target_idx: The index position of the target variable in data. Data\n",
    "                        is a 2D tensor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.indices = indices\n",
    "        self.data = data\n",
    "        print(\"From get_src_trg: data size = {}\".format(data.size()))\n",
    "        self.enc_seq_len = enc_seq_len\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "        self.target_seq_len = target_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple with 3 elements:\n",
    "        1) src (the encoder input)\n",
    "        2) trg (the decoder input)\n",
    "        3) trg_y (the target)\n",
    "        \"\"\"\n",
    "        # Get the first element of the i'th tuple in the list self.indicesasdfas\n",
    "        start_idx = self.indices[index][0]\n",
    "        # Get the second (and last) element of the i'th tuple in the list self.indices\n",
    "        end_idx = self.indices[index][1]\n",
    "        sequence = self.data[start_idx:end_idx]\n",
    "        src, trg, trg_y = self.get_src_trg(\n",
    "            sequence=sequence,\n",
    "            enc_seq_len=self.enc_seq_len,\n",
    "            dec_seq_len=self.dec_seq_len,\n",
    "            target_seq_len=self.target_seq_len\n",
    "            )\n",
    "\n",
    "        return src, trg, trg_y\n",
    "    \n",
    "    def get_src_trg(\n",
    "        self,\n",
    "        sequence: torch.Tensor, \n",
    "        enc_seq_len: int, \n",
    "        dec_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Generate the src (encoder input), trg (decoder input) and trg_y (the target)\n",
    "        sequences from a sequence. \n",
    "\n",
    "        Args:\n",
    "\n",
    "            sequence: tensor, a 1D tensor of length n where \n",
    "                    n = encoder input length + target sequence length  \n",
    "\n",
    "            enc_seq_len: int, the desired length of the input to the transformer encoder\n",
    "\n",
    "            target_seq_len: int, the desired length of the target sequence (the \n",
    "                            one against which the model output is compared)\n",
    "\n",
    "        Return: \n",
    "\n",
    "            src: tensor, 1D, used as input to the transformer model\n",
    "\n",
    "            trg: tensor, 1D, used as input to the transformer model\n",
    "\n",
    "            trg_y: tensor, 1D, the target sequence against which the model output\n",
    "                is compared when computing loss. \n",
    "        \n",
    "        \"\"\"\n",
    "        assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "        \n",
    "        # encoder input\n",
    "        src = sequence[:enc_seq_len] \n",
    "        \n",
    "        # decoder input. As per the paper, it must have the same dimension as the \n",
    "        # target sequence, and it must contain the last value of src, and all\n",
    "        # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
    "        trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "        \n",
    "        assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "        # The target sequence against which the model output will be compared to compute loss\n",
    "        trg_y = sequence[-target_seq_len:]\n",
    "        assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "        return src, trg, trg_y.squeeze(-1) # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len] \n",
    "def generate_square_subsequent_mask(dim1: int, dim2: int):\n",
    "    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_encoder_decoder_inference(\n",
    "    model: nn.Module, \n",
    "    src: torch.Tensor, \n",
    "    forecast_window: int,\n",
    "    batch_size: int,\n",
    "    device,\n",
    "    batch_first: bool=False\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "\n",
    "    # Dimension of a batched model input that contains the target sequence values\n",
    "    target_seq_dim = 0 if batch_first == False else 1\n",
    "\n",
    "    # Take the last value of thetarget variable in all batches in src and make it tgt\n",
    "    # as per the Influenza paper\n",
    "    tgt = src[-1, :, 0] if batch_first == False else src[:, -1, 0] # shape [1, batch_size, 1]\n",
    "\n",
    "    # Change shape from [batch_size] to [1, batch_size, 1]\n",
    "    if batch_size == 1 and batch_first == False:\n",
    "        tgt = tgt.unsqueeze(0).unsqueeze(0) # change from [1] to [1, 1, 1]\n",
    "\n",
    "    # Change shape from [batch_size] to [1, batch_size, 1]\n",
    "    if batch_first == False and batch_size > 1:\n",
    "        tgt = tgt.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "    # Iteratively concatenate tgt with the first element in the prediction\n",
    "    for _ in range(forecast_window-1):\n",
    "\n",
    "        # Create masks\n",
    "        dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
    "\n",
    "        dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(\n",
    "            dim1=dim_a,\n",
    "            dim2=dim_a\n",
    "            ).to(device)\n",
    "\n",
    "        src_mask = generate_square_subsequent_mask(\n",
    "            dim1=dim_a,\n",
    "            dim2=dim_b\n",
    "            ).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model(src, tgt, src_mask, tgt_mask) \n",
    "\n",
    "        # If statement simply makes sure that the predicted value is \n",
    "        # extracted and reshaped correctly\n",
    "        if batch_first == False:\n",
    "\n",
    "            # Obtain the predicted value at t+1 where t is the last time step \n",
    "            # represented in tgt\n",
    "            last_predicted_value = prediction[-1, :, :] \n",
    "\n",
    "            # Reshape from [batch_size, 1] --> [1, batch_size, 1]\n",
    "            last_predicted_value = last_predicted_value.unsqueeze(0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Obtain predicted value\n",
    "            last_predicted_value = prediction[:, -1, :]\n",
    "\n",
    "            # Reshape from [batch_size, 1] --> [batch_size, 1, 1]\n",
    "            last_predicted_value = last_predicted_value.unsqueeze(-1)\n",
    "\n",
    "        # Detach the predicted element from the graph and concatenate with \n",
    "        # tgt in dimension 1 or 0\n",
    "        tgt = torch.cat((tgt, last_predicted_value.detach()), target_seq_dim)\n",
    "    \n",
    "    # Create masks\n",
    "    dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
    "\n",
    "    dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(\n",
    "        dim1=dim_a,\n",
    "        dim2=dim_a\n",
    "        ).to(device)\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(\n",
    "        dim1=dim_a,\n",
    "        dim2=dim_b\n",
    "        ).to(device)\n",
    "\n",
    "    # Make final prediction\n",
    "    final_prediction = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "test_size = 0.1\n",
    "batch_size = 32\n",
    "target_col_name = \"Normalized Demand\"\n",
    "timestamp_col = \"timestamp\"\n",
    "# Only use data from this date and onwards\n",
    "cutoff_date = datetime.datetime(2017, 1, 1) \n",
    "\n",
    "## Params\n",
    "dim_val = 128 #512\n",
    "n_heads = 8 #8\n",
    "n_decoder_layers = 4#4\n",
    "n_encoder_layers = 4#4\n",
    "dec_seq_len = 32 # length of input given to decoder\n",
    "enc_seq_len = 32 # length of input given to encoder\n",
    "output_sequence_length = 24 # target sequence length. If hourly data and length = 48, you predict 2 days ahead\n",
    "window_size = enc_seq_len + output_sequence_length # used to slice data into sub-sequences\n",
    "step_size = 1 # Step size, i.e. how many time steps does the moving window move at each step\n",
    "in_features_encoder_linear_layer = 512 #2048\n",
    "in_features_decoder_linear_layer = 512 #2048\n",
    "max_seq_len = enc_seq_len\n",
    "batch_first = False\n",
    "\n",
    "# Define input variables \n",
    "exogenous_vars = [] # should contain strings. Each string must correspond to a column name\n",
    "input_variables = [target_col_name] + exogenous_vars\n",
    "target_idx = 0 # index position of target in batched trg_y\n",
    "\n",
    "input_size = len(input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidanwiteck/.virtualenvs/thesis/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = TimeSeriesTransformer(\n",
    "    input_size=input_size,\n",
    "    dec_seq_len=enc_seq_len,\n",
    "    batch_first=batch_first,\n",
    "    num_predicted_features=1\n",
    "    )\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (encoder_input_layer): Linear(in_features=1, out_features=512, bias=True)\n",
       "  (decoder_input_layer): Linear(in_features=1, out_features=512, bias=True)\n",
       "  (linear_mapping): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (positional_encoding_layer): PositionalEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"\"\n",
    "model.load_state_dict(torch.load(\"../models/trained_models/20231119.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "path = \"/Users/aidanwiteck/Desktop/Princeton/Year 4/Thesis/electricgrid_new/data/final_tables/banc/banc.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Z-score normalization\n",
    "mean_demand = df['Demand (MWh)'].mean()\n",
    "std_demand = df['Demand (MWh)'].std()\n",
    "\n",
    "df['Normalized Demand'] = (df['Demand (MWh)'] - mean_demand) / std_demand\n",
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices_entire_sequence(data, window_size: int, step_size: int) -> list:\n",
    "    \"\"\"\n",
    "    Produce all the start and end index positions that is needed to produce\n",
    "    the sub-sequences. \n",
    "    \"\"\"\n",
    "\n",
    "    stop_position = len(data)-1 # 1- because of 0 indexing\n",
    "    # Start the first sub-sequence at index position 0\n",
    "    subseq_first_idx = 0\n",
    "    subseq_last_idx = window_size\n",
    "    indices = []\n",
    "    while subseq_last_idx <= stop_position:\n",
    "        indices.append((subseq_first_idx, subseq_last_idx))\n",
    "        subseq_first_idx += step_size\n",
    "        subseq_last_idx += step_size\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From get_src_trg: data size = torch.Size([6578, 1])\n"
     ]
    }
   ],
   "source": [
    "# Remove test data from dataset\n",
    "test_data = data[-(round(len(data)*test_size)):]\n",
    "# Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chunkc. \n",
    "# Should be training data indices only\n",
    "\n",
    "test_indices = get_indices_entire_sequence(\n",
    "    data=test_data, \n",
    "    window_size=window_size, \n",
    "    step_size=step_size)\n",
    "\n",
    "# Making instance of custom dataset class\n",
    "test_data = TransformerDataset(\n",
    "    data=torch.tensor(test_data[input_variables].values).float(),\n",
    "    indices=test_indices,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    target_seq_len=output_sequence_length\n",
    "    )\n",
    "\n",
    "# Making dataloader\n",
    "test_data = DataLoader(test_data, batch_size)\n",
    "i, batch = next(enumerate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         ...,\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815]],\n",
      "\n",
      "        [[-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         ...,\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815]],\n",
      "\n",
      "        [[-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         ...,\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         ...,\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815]],\n",
      "\n",
      "        [[-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         ...,\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815]],\n",
      "\n",
      "        [[-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         ...,\n",
      "         [-0.3815],\n",
      "         [-0.3815],\n",
      "         [-0.3815]]])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all (x,y) pairs in validation dataloader\n",
    "model.eval()\n",
    "forecast_window=48\n",
    "with torch.no_grad():\n",
    "    \n",
    "    \n",
    "    for i, (src, _, tgt_y) in enumerate(test_data):\n",
    "        if i >= 1: \n",
    "            break\n",
    "        prediction = run_encoder_decoder_inference(\n",
    "            model=model, \n",
    "            src=src, \n",
    "            forecast_window=forecast_window,\n",
    "            batch_size=src.shape[1],\n",
    "            device = device\n",
    "            )\n",
    "        print(prediction)\n",
    "\n",
    "#         loss = criterion(tgt_y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.3815), tensor(-0.3815))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(prediction), torch.max(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
