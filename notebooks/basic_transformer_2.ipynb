{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------\n",
    "EPOCHS = 10\n",
    "LR = 0.001\n",
    "SEQ_LENGTH = 12 # Number of historical data points to consider\n",
    "BATCH_SIZE = 64\n",
    "D_MODEL = 1 #2  # number of features (demand + temperature)\n",
    "NHEAD = 1\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "# -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sequences(data, seq_length, num_samples):\n",
    "#     sequences = []\n",
    "#     target = []\n",
    "#     if (num_samples > len(data)):\n",
    "#         print(\"num_samples too large\")\n",
    "#         return\n",
    "    \n",
    "#     for _ in range(num_samples):\n",
    "#         idx = random.randint(0, len(data)-seq_length - 1)\n",
    "#         seq = data[idx:idx+seq_length+1]\n",
    "# #         label = data[idx+seq_length]\n",
    "#         sequences.append(seq)\n",
    "#         target.append(seq)\n",
    "# #         target.append(label)\n",
    "\n",
    "#     return np.array(sequences), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, num_samples):\n",
    "    res = []\n",
    "    if (num_samples > len(data)):\n",
    "        print(\"num_samples too large\")\n",
    "        return\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        idx = random.randint(0, len(data)-seq_length - 1)\n",
    "        seq = data[idx:idx+seq_length+1]\n",
    "        res.append([seq, seq])\n",
    "    return res\n",
    "\n",
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "            batches.append(np.array(data[idx : idx + batch_size]))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data into a DataFrame\n",
    "path = \"/Users/aidanwiteck/Desktop/Princeton/Year 4/Thesis/electricgrid/data/final_tables/banc/banc.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Z-score normalization\n",
    "mean_demand = df['Demand (MWh)'].mean()\n",
    "std_demand = df['Demand (MWh)'].std()\n",
    "\n",
    "df['Normalized Demand'] = (df['Demand (MWh)'] - mean_demand) / std_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562 batches of size 16\n",
      "187 batches of size 16\n"
     ]
    }
   ],
   "source": [
    "train_data = create_sequences(df['Normalized Demand'].values, SEQ_LENGTH, 9000)\n",
    "val_data = create_sequences(df['Normalized Demand'].values, SEQ_LENGTH, 3000)\n",
    "\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create sequences\n",
    "# X, y = create_sequences(df['Normalized Demand'].values, SEQ_LENGTH, 5000)  # For example, creating 5000 samples\n",
    "# X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)\n",
    "# y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data (80/20 split)\n",
    "# train_size = int(0.8 * len(X))\n",
    "# X_train, X_test = X[:train_size], X[train_size:]\n",
    "# y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) \n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: \n",
    "    https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
    "        )\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p,\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "#         src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "#         tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we\n",
    "        # permute to obtain size (sequence length, batch_size, dim_model),\n",
    "        src = src.permute(1,0,2)\n",
    "        tgt = tgt.permute(1,0,2)\n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer(src, \n",
    "                                           tgt, \n",
    "                                           tgt_mask=tgt_mask, \n",
    "                                           src_key_padding_mask=src_pad_mask, \n",
    "                                           tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_random_data(n):\n",
    "#     SOS_token = np.array([2])\n",
    "#     EOS_token = np.array([3])\n",
    "#     length = 8\n",
    "\n",
    "#     data = []\n",
    "\n",
    "#     # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
    "#     for i in range(n // 3):\n",
    "#         X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "#         y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "#         data.append([X, y])\n",
    "\n",
    "#     # 0,0,0,0 -> 0,0,0,0\n",
    "#     for i in range(n // 3):\n",
    "#         X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "#         y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "#         data.append([X, y])\n",
    "\n",
    "#     # 1,0,1,0 -> 1,0,1,0,1\n",
    "#     for i in range(n // 3):\n",
    "#         X = np.zeros(length)\n",
    "#         start = random.randint(0, 1)\n",
    "\n",
    "#         X[start::2] = 1\n",
    "\n",
    "#         y = np.zeros(length)\n",
    "#         if X[-1] == 0:\n",
    "#             y[::2] = 1\n",
    "#         else:\n",
    "#             y[1::2] = 1\n",
    "\n",
    "#         X = np.concatenate((SOS_token, X, EOS_token))\n",
    "#         y = np.concatenate((SOS_token, y, EOS_token))\n",
    "\n",
    "#         data.append([X, y])\n",
    "\n",
    "#     np.random.shuffle(data)\n",
    "\n",
    "#     print(len(data))\n",
    "#     print(data[123])\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "#     batches = []\n",
    "#     for idx in range(0, len(data), batch_size):\n",
    "#         # We make sure we dont get the last bit if its not batch_size size\n",
    "#         if idx + batch_size < len(data):\n",
    "#             # Here you would need to get the max length of the batch,\n",
    "#             # and normalize the length with the PAD token.\n",
    "#             if padding:\n",
    "#                 max_batch_length = 0\n",
    "\n",
    "#                 # Get longest sentence in batch\n",
    "#                 for seq in data[idx : idx + batch_size]:\n",
    "#                     if len(seq) > max_batch_length:\n",
    "#                         max_batch_length = len(seq)\n",
    "\n",
    "#                 # Append X padding tokens until it reaches the max length\n",
    "#                 for seq_idx in range(batch_size):\n",
    "#                     remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
    "#                     data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "#             batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "#     print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "#     return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = generate_random_data(9000)\n",
    "# val_data = generate_random_data(3000)\n",
    "\n",
    "# train_dataloader = batchify_data(train_data)\n",
    "# val_dataloader = batchify_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.69617821e-02 -2.11041870e-01 -3.60471588e-01 -4.60091401e-01\n",
      "  -5.16398251e-01 -5.56462741e-01 -5.59711213e-01 -5.08818483e-01\n",
      "  -3.70217005e-01 -1.64480436e-01 -3.02102543e-02 -1.39678936e-02\n",
      "  -3.02102543e-02]\n",
      " [-6.11686767e-01 -4.97990242e-01 -2.97667794e-01 -2.31615527e-01\n",
      "  -2.01296454e-01 -1.34161363e-01 -3.77900226e-02  1.09370595e-02\n",
      "   9.75629832e-02  1.82023259e-01  2.76228951e-01  3.46612514e-01\n",
      "   4.22410197e-01]\n",
      " [-5.32640612e-01 -4.49263160e-01 -2.97667794e-01 -2.11041870e-01\n",
      "  -1.35244187e-01 -9.30140490e-02 -9.19312249e-02 -4.10384947e-02\n",
      "   1.52683557e-02 -9.62625211e-02 -6.59434478e-02  1.31027076e-02\n",
      "   6.72439099e-02]\n",
      " [-2.86839553e-01 -2.80342609e-01 -2.26201407e-01 -1.42823955e-01\n",
      "  -6.59434478e-02  6.50782618e-02  2.38330109e-01  4.28907141e-01\n",
      "   6.32478062e-01  8.90190185e-01  1.18471833e+00  1.46950105e+00\n",
      "   1.73262729e+00]\n",
      " [ 1.80940435e-01  1.65780898e-01  1.30047705e-01  7.04923820e-02\n",
      "  -6.38812527e-03 -1.14670530e-01 -2.29449879e-01 -3.12827330e-01\n",
      "  -3.66968533e-01 -4.05950198e-01 -4.16778439e-01 -4.01618902e-01\n",
      "  -3.47477700e-01]\n",
      " [ 1.58211475e+00  1.70555669e+00  1.63300748e+00  1.45542434e+00\n",
      "   1.28758661e+00  9.57325276e-01  5.86999452e-01  2.53489646e-01\n",
      "   2.28481240e-02 -1.31995715e-01 -2.25118583e-01 -2.80342609e-01\n",
      "  -2.64100248e-01]\n",
      " [-4.23275383e-01 -4.30855151e-01 -3.78879597e-01 -2.95502146e-01\n",
      "  -9.19312249e-02 -7.02747440e-02 -1.08173586e-01 -1.69894556e-01\n",
      "  -2.70597193e-01 -4.16778439e-01 -5.48882972e-01 -6.27929128e-01\n",
      "  -6.95064219e-01]\n",
      " [ 1.34172781e+00  1.11108629e+00  8.99935601e-01  5.78336860e-01\n",
      "   2.29667517e-01  1.08819010e-04 -1.79639973e-01 -2.87922377e-01\n",
      "  -3.63720061e-01 -3.72382653e-01 -3.21489923e-01 -2.93336498e-01\n",
      "  -1.66646084e-01]\n",
      " [-6.01941351e-01 -6.25763480e-01 -6.05189823e-01 -5.77036398e-01\n",
      "  -5.04487187e-01 -4.49263160e-01 -3.85376541e-01 -3.03081914e-01\n",
      "  -2.95502146e-01 -3.03081914e-01 -3.57223116e-01 -3.47477700e-01\n",
      "  -3.55057468e-01]\n",
      " [ 4.89545288e-01  5.43686490e-01  5.11201769e-01  4.12664781e-01\n",
      "   2.87057191e-01  4.66702530e-02 -1.67728908e-01 -3.55057468e-01\n",
      "  -4.75250937e-01 -5.73787925e-01 -6.20349359e-01 -6.32260424e-01\n",
      "  -6.34426072e-01]\n",
      " [ 1.28433814e+00  1.50090295e+00  1.64816702e+00  1.69689410e+00\n",
      "   1.51714531e+00  1.23994235e+00  9.73567637e-01  6.33560886e-01\n",
      "   3.13044968e-01  8.02377984e-02 -9.95109932e-02 -2.34863999e-01\n",
      "  -3.20407099e-01]\n",
      " [-3.66968533e-01 -2.63017424e-01 -1.63397612e-01 -7.02747440e-02\n",
      "   3.80076606e-02  1.30047705e-01  2.65400710e-01  4.06167836e-01\n",
      "   5.86999452e-01  7.58085651e-01  9.06432546e-01  9.50828332e-01\n",
      "   8.65285232e-01]\n",
      " [ 2.16673628e-01  1.83106083e-01  1.30047705e-01  3.04278923e-02\n",
      "  -1.30912891e-01 -2.47857888e-01 -3.32318163e-01 -3.74548301e-01\n",
      "  -4.00536078e-01 -3.87542190e-01 -3.23655571e-01 -1.72060204e-01\n",
      "   5.42500213e-02]\n",
      " [ 3.68268995e-01  6.39954377e-02 -1.57983492e-01 -2.90088025e-01\n",
      "  -3.71299829e-01 -4.18944087e-01 -3.85376541e-01 -2.84673905e-01\n",
      "  -7.56888642e-02 -4.22247717e-03  3.58420125e-02  9.43145110e-02\n",
      "   1.67946546e-01]\n",
      " [-2.79259785e-01 -1.60149140e-01 -1.70977380e-01 -1.98047981e-01\n",
      "  -2.65183072e-01 -3.89707838e-01 -3.89707838e-01 -5.65125333e-01\n",
      "  -5.65125333e-01 -6.23597832e-01 -6.64745145e-01 -6.52834081e-01\n",
      "  -6.33343248e-01]\n",
      " [ 1.07308400e-01  2.14507980e-01  3.96422420e-01  5.66425795e-01\n",
      "   8.12226854e-01  1.08618134e+00  1.32656828e+00  1.59402582e+00\n",
      "   1.73371012e+00  1.68065174e+00  1.50739989e+00  1.19013245e+00\n",
      "   9.29171851e-01]]\n",
      "[[-2.69617821e-02 -2.11041870e-01 -3.60471588e-01 -4.60091401e-01\n",
      "  -5.16398251e-01 -5.56462741e-01 -5.59711213e-01 -5.08818483e-01\n",
      "  -3.70217005e-01 -1.64480436e-01 -3.02102543e-02 -1.39678936e-02]\n",
      " [-6.11686767e-01 -4.97990242e-01 -2.97667794e-01 -2.31615527e-01\n",
      "  -2.01296454e-01 -1.34161363e-01 -3.77900226e-02  1.09370595e-02\n",
      "   9.75629832e-02  1.82023259e-01  2.76228951e-01  3.46612514e-01]\n",
      " [-5.32640612e-01 -4.49263160e-01 -2.97667794e-01 -2.11041870e-01\n",
      "  -1.35244187e-01 -9.30140490e-02 -9.19312249e-02 -4.10384947e-02\n",
      "   1.52683557e-02 -9.62625211e-02 -6.59434478e-02  1.31027076e-02]\n",
      " [-2.86839553e-01 -2.80342609e-01 -2.26201407e-01 -1.42823955e-01\n",
      "  -6.59434478e-02  6.50782618e-02  2.38330109e-01  4.28907141e-01\n",
      "   6.32478062e-01  8.90190185e-01  1.18471833e+00  1.46950105e+00]\n",
      " [ 1.80940435e-01  1.65780898e-01  1.30047705e-01  7.04923820e-02\n",
      "  -6.38812527e-03 -1.14670530e-01 -2.29449879e-01 -3.12827330e-01\n",
      "  -3.66968533e-01 -4.05950198e-01 -4.16778439e-01 -4.01618902e-01]\n",
      " [ 1.58211475e+00  1.70555669e+00  1.63300748e+00  1.45542434e+00\n",
      "   1.28758661e+00  9.57325276e-01  5.86999452e-01  2.53489646e-01\n",
      "   2.28481240e-02 -1.31995715e-01 -2.25118583e-01 -2.80342609e-01]\n",
      " [-4.23275383e-01 -4.30855151e-01 -3.78879597e-01 -2.95502146e-01\n",
      "  -9.19312249e-02 -7.02747440e-02 -1.08173586e-01 -1.69894556e-01\n",
      "  -2.70597193e-01 -4.16778439e-01 -5.48882972e-01 -6.27929128e-01]\n",
      " [ 1.34172781e+00  1.11108629e+00  8.99935601e-01  5.78336860e-01\n",
      "   2.29667517e-01  1.08819010e-04 -1.79639973e-01 -2.87922377e-01\n",
      "  -3.63720061e-01 -3.72382653e-01 -3.21489923e-01 -2.93336498e-01]\n",
      " [-6.01941351e-01 -6.25763480e-01 -6.05189823e-01 -5.77036398e-01\n",
      "  -5.04487187e-01 -4.49263160e-01 -3.85376541e-01 -3.03081914e-01\n",
      "  -2.95502146e-01 -3.03081914e-01 -3.57223116e-01 -3.47477700e-01]\n",
      " [ 4.89545288e-01  5.43686490e-01  5.11201769e-01  4.12664781e-01\n",
      "   2.87057191e-01  4.66702530e-02 -1.67728908e-01 -3.55057468e-01\n",
      "  -4.75250937e-01 -5.73787925e-01 -6.20349359e-01 -6.32260424e-01]\n",
      " [ 1.28433814e+00  1.50090295e+00  1.64816702e+00  1.69689410e+00\n",
      "   1.51714531e+00  1.23994235e+00  9.73567637e-01  6.33560886e-01\n",
      "   3.13044968e-01  8.02377984e-02 -9.95109932e-02 -2.34863999e-01]\n",
      " [-3.66968533e-01 -2.63017424e-01 -1.63397612e-01 -7.02747440e-02\n",
      "   3.80076606e-02  1.30047705e-01  2.65400710e-01  4.06167836e-01\n",
      "   5.86999452e-01  7.58085651e-01  9.06432546e-01  9.50828332e-01]\n",
      " [ 2.16673628e-01  1.83106083e-01  1.30047705e-01  3.04278923e-02\n",
      "  -1.30912891e-01 -2.47857888e-01 -3.32318163e-01 -3.74548301e-01\n",
      "  -4.00536078e-01 -3.87542190e-01 -3.23655571e-01 -1.72060204e-01]\n",
      " [ 3.68268995e-01  6.39954377e-02 -1.57983492e-01 -2.90088025e-01\n",
      "  -3.71299829e-01 -4.18944087e-01 -3.85376541e-01 -2.84673905e-01\n",
      "  -7.56888642e-02 -4.22247717e-03  3.58420125e-02  9.43145110e-02]\n",
      " [-2.79259785e-01 -1.60149140e-01 -1.70977380e-01 -1.98047981e-01\n",
      "  -2.65183072e-01 -3.89707838e-01 -3.89707838e-01 -5.65125333e-01\n",
      "  -5.65125333e-01 -6.23597832e-01 -6.64745145e-01 -6.52834081e-01]\n",
      " [ 1.07308400e-01  2.14507980e-01  3.96422420e-01  5.66425795e-01\n",
      "   8.12226854e-01  1.08618134e+00  1.32656828e+00  1.59402582e+00\n",
      "   1.73371012e+00  1.68065174e+00  1.50739989e+00  1.19013245e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader[0][:, 0])\n",
    "print(train_dataloader[0][:, 0][:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(\n",
    "    num_tokens=4, dim_model=8, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1\n",
    ").to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: \n",
    "    https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[:, 0], batch[:, 1]\n",
    "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y_expected)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: \n",
    "    https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[:, 0], batch[:, 1]\n",
    "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, y_expected)\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model\n",
      "------------------------- Epoch 1 -------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (13) must match the size of tensor b (8) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loss_list, validation_loss_list\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# train_loss_list = fit(model, opt, loss_fn, train_loader, 10)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m train_loss_list, validation_loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[125], line 16\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, opt, loss_fn, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m25\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m25\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     train_loss_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [train_loss]\n\u001b[1;32m     19\u001b[0m     validation_loss \u001b[38;5;241m=\u001b[39m validation_loop(model, loss_fn, val_dataloader)\n",
      "Cell \u001b[0;32mIn[123], line 24\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, opt, loss_fn, dataloader)\u001b[0m\n\u001b[1;32m     21\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_tgt_mask(sequence_length)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Standard training except we pass in y_input and tgt_mask\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Permute pred to have batch size first again\u001b[39;00m\n\u001b[1;32m     27\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)      \n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[118], line 44\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, tgt_mask, src_pad_mask, tgt_pad_mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt, tgt_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, src_pad_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tgt_pad_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# Src size must be (batch_size, src sequence length)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# Tgt size must be (batch_size, tgt sequence length)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#         src = self.embedding(src) * math.sqrt(self.dim_model)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#         tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m         src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m         tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoder(tgt)\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m# We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;66;03m# permute to obtain size (sequence length, batch_size, dim_model),\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[117], line 27\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, token_embedding)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_embedding: torch\u001b[38;5;241m.\u001b[39mtensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Residual connection + pos encoding\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[43mtoken_embedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtoken_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (13) must match the size of tensor b (8) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "# def fit(model, opt, loss_fn, train_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: \n",
    "    https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "    \n",
    "# train_loss_list = fit(model, opt, loss_fn, train_loader, 10)\n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
