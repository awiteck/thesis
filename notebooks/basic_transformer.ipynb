{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to make a **very** basic transformer model that predicts load values. For this initial model, I intend to **only** use previous readings as input. That is, **no context variables (weather, time, etc)** will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------\n",
    "EPOCHS = 10\n",
    "LR = 0.001\n",
    "SEQ_LENGTH = 12 # Number of historical data points to consider\n",
    "BATCH_SIZE = 64\n",
    "D_MODEL = 1 #2  # number of features (demand + temperature)\n",
    "NHEAD = 1\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "# -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, num_samples):\n",
    "    sequences = []\n",
    "    target = []\n",
    "    if (num_samples > len(data)):\n",
    "        print(\"num_samples too large\")\n",
    "        return\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        idx = random.randint(0, len(data)-seq_length - 1)\n",
    "        seq = data[idx:idx+seq_length]\n",
    "        label = data[idx+seq_length]\n",
    "        sequences.append(seq)\n",
    "        target.append(label)\n",
    "\n",
    "    return np.array(sequences), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dropout=0.1):\n",
    "        super(TransformerPredictor, self).__init__()\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dropout=dropout),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.transpose(0, 1)  # This transposes the batch and sequence dimensions.\n",
    "#         print(src.shape)\n",
    "        output = self.transformer_encoder(src)  \n",
    "#         print(output.shape)\n",
    "        last_output = output[-1, :, :]\n",
    "#         print(last_output.shape)\n",
    "        prediction = self.fc(last_output)\n",
    "#         print(prediction.shape)\n",
    "#         print(prediction.squeeze(-1).shape)\n",
    "        return prediction.squeeze(-1)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data into a DataFrame\n",
    "path = \"/Users/aidanwiteck/Desktop/Princeton/Year 4/Thesis/electricgrid/data/final_tables/banc/banc.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Z-score normalization\n",
    "mean_demand = df['Demand (MWh)'].mean()\n",
    "std_demand = df['Demand (MWh)'].std()\n",
    "\n",
    "df['Normalized Demand'] = (df['Demand (MWh)'] - mean_demand) / std_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "X, y = create_sequences(df['Normalized Demand'].values, SEQ_LENGTH, 5000)  # For example, creating 5000 samples\n",
    "X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)\n",
    "y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80/20 split)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4000, 12, 1]), torch.Size([4000, 1]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidanwiteck/.virtualenvs/thesis/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Model, Loss, Optimizer\n",
    "model = TransformerPredictor(D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.7171139717102051\n",
      "Epoch 2/10 | Loss: 0.35124412178993225\n",
      "Epoch 3/10 | Loss: 0.15351220965385437\n",
      "Epoch 4/10 | Loss: 0.4189613163471222\n",
      "Epoch 5/10 | Loss: 0.36694180965423584\n",
      "Epoch 6/10 | Loss: 0.3504642844200134\n",
      "Epoch 7/10 | Loss: 0.14471067488193512\n",
      "Epoch 8/10 | Loss: 24.719085693359375\n",
      "Epoch 9/10 | Loss: 0.404633104801178\n",
      "Epoch 10/10 | Loss: 0.3308394253253937\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs).unsqueeze(-1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} | Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_load(model, input_sequence, num_predictions):\n",
    "    \"\"\"\n",
    "    Generate load values using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained model\n",
    "    - input_sequence: A starting sequence to use for generation\n",
    "    - num_predictions: Number of future load values to predict\n",
    "\n",
    "    Returns:\n",
    "    A list of predicted load values.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(num_predictions):\n",
    "        with torch.no_grad():\n",
    "            prediction = model(input_sequence)\n",
    "            print(f\"Prediction shape: {prediction.shape}\")  # Add this line\n",
    "            predictions.append(prediction.item())\n",
    "\n",
    "            # Update the input_sequence for the next prediction\n",
    "            input_sequence = torch.roll(input_sequence, shifts=-1, dims=0)\n",
    "            input_sequence[-1, 0] = prediction\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5240],\n",
      "         [-0.3843],\n",
      "         [-0.2858],\n",
      "         [-0.2673],\n",
      "         [-0.2998],\n",
      "         [-0.3226],\n",
      "         [-0.3670],\n",
      "         [-0.3594],\n",
      "         [-0.3735],\n",
      "         [-0.3594],\n",
      "         [-0.3464],\n",
      "         [-0.2793]]])\n",
      "Prediction shape: torch.Size([1])\n",
      "Prediction shape: torch.Size([1])\n",
      "Prediction shape: torch.Size([1])\n",
      "Prediction shape: torch.Size([1])\n",
      "Prediction shape: torch.Size([1])\n",
      "Prediction shape: torch.Size([1])\n",
      "Prediction shape: torch.Size([1])\n",
      "Prediction shape: torch.Size([1])\n",
      "Prediction shape: torch.Size([1])\n",
      "Prediction shape: torch.Size([1])\n",
      "[0.00014697760343551636, 0.00014697760343551636, 0.00014697760343551636, 0.00014697760343551636, 0.00014697760343551636, 0.00014697760343551636, 0.00014697760343551636, 0.00014697760343551636, 0.00014697760343551636, 0.00014697760343551636]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you've trained the model\n",
    "\n",
    "# Let's predict the next 10 values using the last sequence from X_test as a starting point\n",
    "starting_sequence = X_test[3].unsqueeze(0)  # unsqueeze to add batch dimension\n",
    "print(starting_sequence)\n",
    "num_predictions = 10\n",
    "\n",
    "predicted_load_values = generate_load(model, starting_sequence, num_predictions)\n",
    "print(predicted_load_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_data, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m test_loss, predictions, true_values \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# If you want other metrics, you can compute them using `predictions` and `true_values`.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# For example, you could calculate the MAE, RMSE, etc.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 8\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, test_loader, criterion)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m----> 8\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(X_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m, in \u001b[0;36mTransformerPredictor.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src):\n\u001b[1;32m     12\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(src)  \u001b[38;5;66;03m# Shape: (SEQ_LENGTH, BATCH_SIZE, D_MODEL)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     last_output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m          \u001b[38;5;66;03m# Shape: (BATCH_SIZE, D_MODEL)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(last_output)       \u001b[38;5;66;03m# Shape: (BATCH_SIZE, 1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    predictions, true_values = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch[0])\n",
    "            print(X_batch.shape)\n",
    "            print(outputs.shape)\n",
    "            print(y_batch.shape)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "\n",
    "            predictions.extend(outputs.numpy())\n",
    "            true_values.extend(y_batch.numpy())\n",
    "\n",
    "    # Calculate the average loss over the entire test dataset\n",
    "    average_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    return average_test_loss, predictions, true_values\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_data = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, predictions, true_values = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# If you want other metrics, you can compute them using `predictions` and `true_values`.\n",
    "# For example, you could calculate the MAE, RMSE, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Demand (MWh)</th>\n",
       "      <th>Demand Forecast (MWh)</th>\n",
       "      <th>Net Generation (MWh)</th>\n",
       "      <th>Region</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-07-01 01:00:00</td>\n",
       "      <td>2513.0</td>\n",
       "      <td>2226.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>banc</td>\n",
       "      <td>38.5</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-07-01 02:00:00</td>\n",
       "      <td>2275.0</td>\n",
       "      <td>2035.0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>banc</td>\n",
       "      <td>37.7</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-07-01 03:00:00</td>\n",
       "      <td>2104.0</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>banc</td>\n",
       "      <td>35.4</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-07-01 04:00:00</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>banc</td>\n",
       "      <td>32.5</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-07-01 05:00:00</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>1811.0</td>\n",
       "      <td>1334.0</td>\n",
       "      <td>banc</td>\n",
       "      <td>30.3</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65779</th>\n",
       "      <td>2022-12-31 20:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1539.0</td>\n",
       "      <td>banc</td>\n",
       "      <td>12.5</td>\n",
       "      <td>92</td>\n",
       "      <td>100</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65780</th>\n",
       "      <td>2022-12-31 21:00:00</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1503.0</td>\n",
       "      <td>banc</td>\n",
       "      <td>12.3</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65781</th>\n",
       "      <td>2022-12-31 22:00:00</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>1862.0</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>banc</td>\n",
       "      <td>12.1</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65782</th>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>1784.0</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>banc</td>\n",
       "      <td>12.1</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65783</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>1594.0</td>\n",
       "      <td>1717.0</td>\n",
       "      <td>1602.0</td>\n",
       "      <td>banc</td>\n",
       "      <td>11.8</td>\n",
       "      <td>89</td>\n",
       "      <td>100</td>\n",
       "      <td>24.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65784 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  Demand (MWh)  Demand Forecast (MWh)  \\\n",
       "0     2015-07-01 01:00:00        2513.0                 2226.0   \n",
       "1     2015-07-01 02:00:00        2275.0                 2035.0   \n",
       "2     2015-07-01 03:00:00        2104.0                 1897.0   \n",
       "3     2015-07-01 04:00:00        1988.0                 1821.0   \n",
       "4     2015-07-01 05:00:00        1958.0                 1811.0   \n",
       "...                   ...           ...                    ...   \n",
       "65779 2022-12-31 20:00:00        2025.0                 2021.0   \n",
       "65780 2022-12-31 21:00:00        1821.0                 1941.0   \n",
       "65781 2022-12-31 22:00:00        1686.0                 1862.0   \n",
       "65782 2022-12-31 23:00:00        1625.0                 1784.0   \n",
       "65783 2023-01-01 00:00:00        1594.0                 1717.0   \n",
       "\n",
       "       Net Generation (MWh) Region  temperature  humidity  cloudcover  \\\n",
       "0                    1559.0   banc         38.5        18          10   \n",
       "1                    1441.0   banc         37.7        19          12   \n",
       "2                    1399.0   banc         35.4        23          11   \n",
       "3                    1354.0   banc         32.5        27          16   \n",
       "4                    1334.0   banc         30.3        31          21   \n",
       "...                     ...    ...          ...       ...         ...   \n",
       "65779                1539.0   banc         12.5        92         100   \n",
       "65780                1503.0   banc         12.3        91         100   \n",
       "65781                1488.0   banc         12.1        91         100   \n",
       "65782                1570.0   banc         12.1        91         100   \n",
       "65783                1602.0   banc         11.8        89         100   \n",
       "\n",
       "       windspeed  \n",
       "0            8.7  \n",
       "1           10.4  \n",
       "2           10.7  \n",
       "3           11.4  \n",
       "4            9.0  \n",
       "...          ...  \n",
       "65779       11.7  \n",
       "65780       14.0  \n",
       "65781       17.2  \n",
       "65782       19.6  \n",
       "65783       24.2  \n",
       "\n",
       "[65784 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
